{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aunghlaingtun/NYP_ITI_105_MLPrj/blob/main/NYP_ITI_105_Model_Training_Enhanced_ProV3_0_finalRun28.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlf9M9r61C_-"
      },
      "source": [
        "# ITI-105 Machine Learning Project: HDB Resale Price Prediction\n",
        "## Enhanced Model Training with Comprehensive Hyperparameter Tuning & MLflow\n",
        "\n",
        "**Student Information:**\n",
        "- **Name:** Aung Hlaing Tun\n",
        "- **Student ID:** 6319250G\n",
        "- **Project Group ID:** AlogoRiddler\n",
        "- **Date:** 28 Aug 2025\n",
        "\n",
        "**Enhanced Features:**\n",
        "- ‚úÖ **3-Step Model Training Workflow**\n",
        "- ‚úÖ **Comprehensive Hyperparameter Tuning** (GridSearchCV + RandomizedSearchCV)\n",
        "- ‚úÖ **Professional MLflow Integration** (No Warnings)\n",
        "- ‚úÖ **Dataset Logging & Tracking**\n",
        "- ‚úÖ **Model Comparison Dashboard**\n",
        "- ‚úÖ **Performance Optimization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "installation"
      },
      "source": [
        "## 1. Installation & Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_cell"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install mlflow pyngrok geopy scikit-learn -q\n",
        "\n",
        "print(\"‚úÖ All packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1 ‚Äî install for DagsHub (replace your original install cell)\n",
        "!pip uninstall -y mlflow -q\n",
        "!pip install \"mlflow<3\" dagshub geopy scikit-learn -q\n",
        "\n",
        "import mlflow, sklearn\n",
        "print(\"‚úÖ All packages installed successfully!\")\n",
        "print(\"üìà MLflow version:\", mlflow.__version__)   # must be 2.x for DagsHub\n",
        "print(\"ü§ñ scikit-learn version:\", sklearn.__version__)\n"
      ],
      "metadata": {
        "id": "R-Px1EFvOGEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## 2. Google Drive & Ngrok Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_cell"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from google.colab import userdata, drive, files\n",
        "from pyngrok import ngrok\n",
        "import mlflow\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Mount Google Drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "# Configure ngrok with your auth token\n",
        "NGROK_TOKEN = userdata.get('test2')  # Store in Colab secrets\n",
        "!ngrok authtoken {NGROK_TOKEN}\n",
        "\n",
        "print(\"‚úÖ Google Drive mounted and ngrok configured!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2 ‚Äî DagsHub MLflow setup (replaces Drive + ngrok)\n",
        "from google.colab import userdata\n",
        "import os, mlflow, dagshub\n",
        "\n",
        "# Your repo details\n",
        "USER = \"aunghlaingtun\"      # DagsHub username/org\n",
        "REPO = \"NYP_ITI_105\"        # DagsHub repo name\n",
        "TOKEN = userdata.get('dagshub')  # Colab secret named 'dagshub'\n",
        "\n",
        "assert TOKEN, \"Colab secret 'dagshub' missing. Add it in Runtime ‚Üí Secrets.\"\n",
        "\n",
        "# Auth for DagsHub MLflow (no ngrok needed)\n",
        "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = USER\n",
        "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = TOKEN\n",
        "\n",
        "# Point MLflow to your hosted DagsHub tracking server\n",
        "dagshub.init(repo_owner=USER, repo_name=REPO, mlflow=True)\n",
        "mlflow.set_tracking_uri(f\"https://dagshub.com/{USER}/{REPO}.mlflow\")\n",
        "\n",
        "# Use a consistent experiment name\n",
        "mlflow.set_experiment(\"HDB_Linear_Workflow\")\n",
        "\n",
        "print(\"‚úÖ DagsHub MLflow configured\")\n",
        "print(\"Tracking URI ->\", mlflow.get_tracking_uri())\n",
        "print(\"Open experiments:\", f\"https://dagshub.com/{USER}/{REPO}.mlflow/#/experiments\")\n"
      ],
      "metadata": {
        "id": "0Bd8pNmdOlH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imports"
      },
      "source": [
        "## 3. Import Libraries & Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports_cell"
      },
      "outputs": [],
      "source": [
        "# Data manipulation and analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as asns\n",
        "plt.style.use('default')\n",
        "asns.set_palette(\"husl\")\n",
        "\n",
        "# Machine Learning\n",
        "import sklearn\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split, GridSearchCV, RandomizedSearchCV,\n",
        "    cross_val_score\n",
        ")\n",
        "from sklearn.experimental import enable_halving_search_cv # Import experimental modules\n",
        "from sklearn.model_selection import (\n",
        "    HalvingGridSearchCV, HalvingRandomSearchCV\n",
        ")\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "\n",
        "\n",
        "# MLflow for experiment tracking\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from mlflow.models.signature import infer_signature\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n",
        "print(f\"üìä Pandas version: {pd.__version__}\")\n",
        "print(f\"ü§ñ Scikit-learn version: {sklearn.__version__}\")\n",
        "print(f\"üìà MLflow version: {mlflow.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlflow_setup"
      },
      "source": [
        "## 4. Enhanced MLflow Setup with 3-Step Workflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ DagsHub version of your MLflow setup cell (replaces Drive + ngrok)\n",
        "import os, mlflow, dagshub\n",
        "from google.colab import userdata  # using your Colab secret\n",
        "\n",
        "def setup_dagshub_mlflow_3step():\n",
        "    \"\"\"Configure MLflow to log to your DagsHub repo and create the 3 workflow experiments.\"\"\"\n",
        "    print(\"üöÄ DagsHub MLflow SETUP ‚Äì 3-STEP WORKFLOW\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # === Auth from Colab Secrets ===\n",
        "    USER = \"aunghlaingtun\"       # your DagsHub username/org\n",
        "    REPO = \"NYP_ITI_105\"         # your repo\n",
        "    TOKEN = userdata.get(\"dagshub\")  # Colab secret key: dagshub\n",
        "    assert TOKEN, \"Colab secret 'dagshub' missing. Add it in Runtime ‚Üí Secrets.\"\n",
        "\n",
        "    os.environ[\"MLFLOW_TRACKING_USERNAME\"] = USER\n",
        "    os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = TOKEN\n",
        "\n",
        "    # === Point MLflow to hosted DagsHub tracking server (no ngrok needed) ===\n",
        "    dagshub.init(repo_owner=USER, repo_name=REPO, mlflow=True)\n",
        "    mlflow.set_tracking_uri(f\"https://dagshub.com/{USER}/{REPO}.mlflow\")\n",
        "    print(\"‚úì Tracking URI:\", mlflow.get_tracking_uri())\n",
        "\n",
        "    # === Create/get Step 1/2/3 experiments on DagsHub ===\n",
        "    exp_names = {\n",
        "        \"baseline\":   \"HDB_Step1_Baseline_Models\",\n",
        "        \"tuning\":     \"HDB_Step2_Hyperparameter_Tuning\",\n",
        "        \"comparison\": \"HDB_Step3_Model_Comparison\",\n",
        "    }\n",
        "    experiments = {}\n",
        "    for key, name in exp_names.items():\n",
        "        exp = mlflow.get_experiment_by_name(name)\n",
        "        exp_id = exp.experiment_id if exp else mlflow.create_experiment(name)\n",
        "        experiments[key] = exp_id\n",
        "        print(f\"‚úì {name} (ID: {exp_id})\")\n",
        "\n",
        "    print(\"üîó Hosted MLflow UI:\",\n",
        "          f\"https://dagshub.com/{USER}/{REPO}.mlflow/#/experiments\")\n",
        "    return experiments\n",
        "\n",
        "def start_mlflow_ui_enhanced():\n",
        "    \"\"\"Not needed on DagsHub ‚Äî the UI is hosted. Return the URL for convenience.\"\"\"\n",
        "    USER = \"aunghlaingtun\"\n",
        "    REPO = \"NYP_ITI_105\"\n",
        "    url = f\"https://dagshub.com/{USER}/{REPO}.mlflow/#/experiments\"\n",
        "    print(\"‚ÑπÔ∏è No local UI/ngrok required. Use the hosted URL below.\")\n",
        "    print(\"üåê\", url)\n",
        "    return url\n",
        "\n",
        "# Run setup (same pattern as your original code)\n",
        "experiments = setup_dagshub_mlflow_3step()\n",
        "mlflow_url  = start_mlflow_ui_enhanced()\n",
        "\n",
        "if experiments:\n",
        "    print(\"\\n‚úÖ DagsHub MLflow setup complete!\")\n",
        "    print(\"üìä 3 Experiments created for workflow\")\n",
        "    print(\"üåê Hosted MLflow UI:\", mlflow_url)\n"
      ],
      "metadata": {
        "id": "UlCNaifqPfM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_upload"
      },
      "source": [
        "## 5. Data Upload & Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This is formatted as code\n",
        "\n",
        "```python\n",
        "# Upload your processed data or use the data from the previous notebook\n",
        "print(\"üìÅ Please upload your processed HDB dataset:\")\n",
        "print(\"   - Use the final_data from your previous notebook\")\n",
        "print(\"   - Or upload the processed CSV file\")\n",
        "\n",
        "# If uploading files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# For this example, we'll assume you have the data ready\n",
        "# Replace this with your actual data loading\n",
        "print(\"\\n‚ö†Ô∏è  Please ensure you have:\")\n",
        "print(\"   - X: Feature matrix (preprocessed)\")\n",
        "print(\"   - y: Target variable (resale_price)\")\n",
        "print(\"   - feature_columns: List of feature names\")\n",
        "print(\"\\nüìù Run your data preprocessing from the previous notebook first!\")\n",
        "```\n",
        "\n",
        "**OutPut**\n",
        "# This is formatted as code\n",
        "```python\n",
        "\n",
        "üìÅ Please upload your processed HDB dataset:\n",
        "   - Use the final_data from your previous notebook\n",
        "   - Or upload the processed CSV file\n",
        "hdb_processed_data (1).csv\n",
        "hdb_processed_data (1).csv(text/csv) - 63731377 bytes, last modified: 26/08/2025 - 100% done\n",
        "Saving hdb_processed_data (1).csv to hdb_processed_data (1).csv\n",
        "\n",
        "‚ö†Ô∏è  Please ensure you have:\n",
        "   - X: Feature matrix (preprocessed)\n",
        "   - y: Target variable (resale_price)\n",
        "   - feature_columns: List of feature names\n",
        "\n",
        "üìù Run your data preprocessing from the previous notebook first!\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lxYjXEZ1jYN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "# Assuming 'hdb_processed_data (1).csv' is the file you uploaded and it is in the content directory\n",
        "file_path = '/content/hdb_processed_data.csv'\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Replace 'resale_price' with your actual target column name if different\n",
        "    y = df['resale_price']\n",
        "    X = df.drop('resale_price', axis=1)\n",
        "    feature_columns = X.columns.tolist()\n",
        "\n",
        "    print(\"‚úÖ Data loaded successfully!\")\n",
        "    print(f\"Features shape: {X.shape}\")\n",
        "    print(f\"Target shape: {y.shape}\")\n",
        "    print(f\"Number of features: {len(feature_columns)}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå Error: File not found at {file_path}. Please ensure the file is uploaded or in the correct path.\")\n",
        "except KeyError as e:\n",
        "    print(f\"‚ùå Error: Missing expected column in the DataFrame. Details: {e}\")\n",
        "    print(\"Please check if 'resale_price' or another key column exists in your CSV.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå An unexpected error occurred during data loading: {e}\")"
      ],
      "metadata": {
        "id": "a3sOun3vU6Vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enhanced_logging"
      },
      "source": [
        "## 6. Enhanced MLflow Logging Functions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced MLflow logging (DagsHub / MLflow 2.x compatible)\n",
        "import os, shutil, time\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import mlflow, mlflow.sklearn\n",
        "from mlflow.models.signature import infer_signature\n",
        "\n",
        "def log_model_enhanced_professional(\n",
        "    model, model_name, params, metrics,\n",
        "    X_train_sample, y_train_sample,\n",
        "    feature_names, experiment_id,\n",
        "    run_name_prefix=\"Enhanced\",\n",
        "    log_datasets=True\n",
        "):\n",
        "    \"\"\"Enhanced professional model logging with dataset tracking (no warnings; DagsHub-ready).\"\"\"\n",
        "\n",
        "    # ---- helpers ------------------------------------------------------------\n",
        "    def _to_frame(X, cols):\n",
        "        \"\"\"Return a DataFrame for signature/artifacts; fix/trim col names if needed.\"\"\"\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            return X\n",
        "        X = np.asarray(X)\n",
        "        if cols is None or len(cols) != X.shape[1]:\n",
        "            cols = [f\"f{i}\" for i in range(X.shape[1])]\n",
        "        return pd.DataFrame(X, columns=cols)\n",
        "\n",
        "    def _to_series(y, name=\"resale_price\"):\n",
        "        if isinstance(y, pd.Series):\n",
        "            return y.rename(name)\n",
        "        y = np.asarray(y).ravel()\n",
        "        return pd.Series(y, name=name)\n",
        "\n",
        "    def _log_params_safe(d):\n",
        "        for k, v in (d or {}).items():\n",
        "            try:\n",
        "                mlflow.log_param(k, v if isinstance(v, (str, int, float, bool)) else str(v))\n",
        "            except Exception:\n",
        "                mlflow.log_param(k, str(v))\n",
        "\n",
        "    def _log_metrics_safe(d):\n",
        "        for k, v in (d or {}).items():\n",
        "            try:\n",
        "                mlflow.log_metric(k, float(v))\n",
        "            except Exception:\n",
        "                # skip non-numeric or NaN metrics silently\n",
        "                pass\n",
        "\n",
        "    def _coef_dataframe(estimator, cols):\n",
        "        \"\"\"Return DataFrame of coefficients if available; align lengths safely.\"\"\"\n",
        "        if not hasattr(estimator, \"coef_\"):\n",
        "            return None\n",
        "        coefs = np.asarray(estimator.coef_).ravel()\n",
        "        n = min(len(coefs), len(cols or []))\n",
        "        if n == 0:\n",
        "            # fallback generic names if none provided\n",
        "            cols = [f\"f{i}\" for i in range(len(coefs))]\n",
        "            n = len(coefs)\n",
        "        df = pd.DataFrame({\n",
        "            \"feature\": (cols[:n] if cols else [f\"f{i}\" for i in range(n)]),\n",
        "            \"coefficient\": coefs[:n]\n",
        "        })\n",
        "        df[\"abs_coefficient\"] = df[\"coefficient\"].abs()\n",
        "        return df.sort_values(\"abs_coefficient\", ascending=False)\n",
        "\n",
        "    # ---- run setup ----------------------------------------------------------\n",
        "    timestamp = int(time.time())\n",
        "    run_name = f\"{run_name_prefix}_{model_name}_{timestamp}\"\n",
        "\n",
        "    X_ex = _to_frame(X_train_sample, feature_names)\n",
        "    y_ex = _to_series(y_train_sample, \"resale_price\")\n",
        "    X_ex_small = X_ex.iloc[:5] if len(X_ex) > 5 else X_ex\n",
        "\n",
        "    with mlflow.start_run(experiment_id=experiment_id, run_name=run_name) as run:\n",
        "        # Tags\n",
        "        mlflow.set_tags({\n",
        "            \"model_family\": \"linear_regression\",\n",
        "            \"model_type\": model_name,\n",
        "            \"project\": \"HDB_Price_Prediction_Enhanced\",\n",
        "            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            \"student_id\": \"6319250G\",\n",
        "            \"student_name\": \"Aung_Hlaing_Tun\",\n",
        "            \"workflow_step\": run_name_prefix\n",
        "        })\n",
        "\n",
        "        # Params & metrics (safe casting)\n",
        "        _log_params_safe(params)\n",
        "        _log_metrics_safe(metrics)\n",
        "\n",
        "        # Signature + input example\n",
        "        signature = None\n",
        "        input_example = None\n",
        "        try:\n",
        "            preds_small = model.predict(X_ex_small)\n",
        "            signature = infer_signature(X_ex_small, preds_small)\n",
        "            input_example = X_ex_small\n",
        "            print(\"  ‚úÖ Created model signature and input example\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö†Ô∏è Could not create signature: {e}\")\n",
        "\n",
        "        # Log model (MLflow 2.x: use artifact_path, not name=)\n",
        "        mlflow.sklearn.log_model(\n",
        "            sk_model=model,\n",
        "            artifact_path=\"model\",\n",
        "            signature=signature,\n",
        "            input_example=input_example,\n",
        "            registered_model_name=f\"HDB_Enhanced_{model_name}_{timestamp}\"\n",
        "        )\n",
        "\n",
        "        # Feature importance / coefficients\n",
        "        est_for_coef = model\n",
        "        # If you pass a Pipeline and want inner estimator coefs, uncomment below:\n",
        "        # try:\n",
        "        #     from sklearn.pipeline import Pipeline\n",
        "        #     if isinstance(model, Pipeline):\n",
        "        #         est_for_coef = model.named_steps.get(\"model\", model.steps[-1][1])\n",
        "        # except Exception:\n",
        "        #     pass\n",
        "\n",
        "        fi = _coef_dataframe(est_for_coef, feature_names)\n",
        "        if fi is not None:\n",
        "            fi.to_csv(\"feature_importance.csv\", index=False)\n",
        "            mlflow.log_artifact(\"feature_importance.csv\")\n",
        "\n",
        "            plt.figure(figsize=(14, 10))\n",
        "            top = fi.head(15)\n",
        "            colors = [\"red\" if c < 0 else \"blue\" for c in top[\"coefficient\"]]\n",
        "            bars = plt.barh(range(len(top)), top[\"coefficient\"], color=colors, alpha=0.85)\n",
        "            plt.yticks(range(len(top)), top[\"feature\"])\n",
        "            plt.xlabel(\"Coefficient Value\")\n",
        "            plt.title(f\"Top 15 Feature Coefficients - {model_name}\")\n",
        "            plt.axvline(0, color=\"black\", linestyle=\"-\", alpha=0.3)\n",
        "            plt.grid(axis=\"x\", alpha=0.25)\n",
        "            for b in bars:\n",
        "                w = b.get_width()\n",
        "                plt.text(w + (0.01*abs(w) if w != 0 else 0.01),\n",
        "                         b.get_y() + b.get_height()/2,\n",
        "                         f\"{w:.0f}\", va=\"center\", fontsize=9)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(\"feature_importance_plot.png\", dpi=300, bbox_inches=\"tight\")\n",
        "            mlflow.log_artifact(\"feature_importance_plot.png\")\n",
        "            plt.close()\n",
        "\n",
        "            # Coef stats\n",
        "            mlflow.log_metric(\"max_abs_coefficient\", float(fi[\"abs_coefficient\"].max()))\n",
        "            mlflow.log_metric(\"mean_abs_coefficient\", float(fi[\"abs_coefficient\"].mean()))\n",
        "            mlflow.log_metric(\"num_positive_coefficients\", float((fi[\"coefficient\"] > 0).sum()))\n",
        "            mlflow.log_metric(\"num_negative_coefficients\", float((fi[\"coefficient\"] < 0).sum()))\n",
        "\n",
        "            # Cleanup\n",
        "            try:\n",
        "                os.remove(\"feature_importance.csv\")\n",
        "                os.remove(\"feature_importance_plot.png\")\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        # Dataset samples as artifacts\n",
        "        if log_datasets:\n",
        "            try:\n",
        "                temp_dir = \"temp_dataset_artifacts\"\n",
        "                os.makedirs(temp_dir, exist_ok=True)\n",
        "                X_ex.to_csv(os.path.join(temp_dir, \"X_train_sample.csv\"), index=False)\n",
        "                y_ex.to_frame().to_csv(os.path.join(temp_dir, \"y_train_sample.csv\"), index=False)\n",
        "                mlflow.log_artifacts(temp_dir, artifact_path=\"datasets\")\n",
        "                shutil.rmtree(temp_dir)\n",
        "                print(\"  ‚úÖ Logged dataset samples\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ‚ö†Ô∏è Could not log datasets: {e}\")\n",
        "\n",
        "        # Estimator extras\n",
        "        for attr in [\"intercept_\", \"alpha\", \"l1_ratio\"]:\n",
        "            if hasattr(model, attr):\n",
        "                try:\n",
        "                    mlflow.log_metric(attr.strip(\"_\") if attr.endswith(\"_\") else attr,\n",
        "                                      float(getattr(model, attr)))\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "        # Nice run links for DagsHub\n",
        "        run_id = run.info.run_id\n",
        "        print(f\"  üéØ Logged to MLflow run: {run_id}\")\n",
        "        uri = mlflow.get_tracking_uri()\n",
        "        try:\n",
        "            # If tracking URI is DagsHub, print deep links\n",
        "            if uri.startswith(\"https://dagshub.com\"):\n",
        "                print(f\"üèÉ View run {run_name} at: {uri}/#/experiments/{experiment_id}/runs/{run_id}\")\n",
        "                print(f\"üß™ View experiment at: {uri}/#/experiments/{experiment_id}\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        return run_id\n",
        "\n",
        "print(\"‚úÖ Enhanced MLflow logging function ready (DagsHub-safe).\")\n"
      ],
      "metadata": {
        "id": "DqjH5VZHXVlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step1"
      },
      "source": [
        "## 7. Step 1: Baseline Model Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline linear models (DagsHub/MLflow-safe)\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "\n",
        "def step1_baseline_models(X, y, feature_names, experiment_id):\n",
        "    \"\"\"Step 1: Train baseline linear models with default parameters and log to MLflow (DagsHub).\"\"\"\n",
        "    print(\"\\nüéØ STEP 1: BASELINE MODEL TRAINING\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # numeric-only (your original approach)\n",
        "    X_numeric = X.select_dtypes(include=np.number)\n",
        "    feature_names_numeric = X_numeric.columns.tolist()\n",
        "\n",
        "    print(f\"üìä Original features: {len(feature_names)}\")\n",
        "    print(f\"üìä Numeric features selected: {len(feature_names_numeric)}\")\n",
        "    print(f\"üìä Numeric features: {feature_names_numeric}\")\n",
        "\n",
        "    if X_numeric.shape[1] == 0:\n",
        "        print(\"‚ùå Error: No numeric features found after dropping non-numeric columns.\")\n",
        "        return None, None, None, None, None, None, None\n",
        "\n",
        "    # split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_numeric, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # scale\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled  = scaler.transform(X_test)\n",
        "\n",
        "    print(f\"üìä Data split: {len(X_train):,} train, {len(X_test):,} test\")\n",
        "    print(\"üìä Feature scaling: StandardScaler applied\")\n",
        "\n",
        "    # models\n",
        "    baseline_models = {\n",
        "        \"LinearRegression\": LinearRegression(),\n",
        "        \"Ridge\": Ridge(random_state=42),\n",
        "        \"Lasso\": Lasso(random_state=42, max_iter=2000),\n",
        "        \"ElasticNet\": ElasticNet(random_state=42, max_iter=2000),\n",
        "    }\n",
        "\n",
        "    # helpers\n",
        "    def safe_mape(y_true, y_pred):\n",
        "        denom = np.clip(np.asarray(y_true), 1e-8, None)\n",
        "        return float(np.mean(np.abs((y_true - y_pred) / denom)) * 100.0)\n",
        "\n",
        "    def _safe_add_model_params(model, params: dict):\n",
        "        if hasattr(model, \"alpha\"):\n",
        "            try: params[\"alpha\"] = float(model.alpha)\n",
        "            except Exception: params[\"alpha\"] = str(model.alpha)\n",
        "        if hasattr(model, \"l1_ratio\"):\n",
        "            try: params[\"l1_ratio\"] = float(model.l1_ratio)\n",
        "            except Exception: params[\"l1_ratio\"] = str(model.l1_ratio)\n",
        "        if hasattr(model, \"max_iter\"):\n",
        "            mi = getattr(model, \"max_iter\", None)\n",
        "            if mi is not None:\n",
        "                try: params[\"max_iter\"] = int(mi)\n",
        "                except Exception: params[\"max_iter\"] = str(mi)\n",
        "\n",
        "    baseline_results = {}\n",
        "\n",
        "    for model_name, model in baseline_models.items():\n",
        "        print(f\"\\nüîß Training baseline {model_name}...\")\n",
        "\n",
        "        start_time = time.time()\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        training_time = float(time.time() - start_time)\n",
        "\n",
        "        # preds\n",
        "        y_pred_train = model.predict(X_train_scaled)\n",
        "        y_pred_test  = model.predict(X_test_scaled)\n",
        "\n",
        "        # metrics\n",
        "        train_r2   = float(r2_score(y_train, y_pred_train))\n",
        "        test_r2    = float(r2_score(y_test,  y_pred_test))\n",
        "        train_rmse = float(np.sqrt(mean_squared_error(y_train, y_pred_train)))\n",
        "        test_rmse  = float(np.sqrt(mean_squared_error(y_test,  y_pred_test)))\n",
        "        train_mae  = float(mean_absolute_error(y_train, y_pred_train))\n",
        "        test_mae   = float(mean_absolute_error(y_test,  y_pred_test))\n",
        "        mape       = safe_mape(y_test, y_pred_test)\n",
        "\n",
        "        # 5-fold CV on training set (scaled)\n",
        "        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring=\"r2\", n_jobs=-1)\n",
        "        cv_mean, cv_std = float(cv_scores.mean()), float(cv_scores.std())\n",
        "\n",
        "        # params to log\n",
        "        params = {\n",
        "            \"model_type\": f\"{model_name}_Baseline\",\n",
        "            \"test_size\": 0.2,\n",
        "            \"random_state\": 42,\n",
        "            \"feature_scaling\": \"StandardScaler\",\n",
        "            \"n_features\": len(feature_names_numeric),\n",
        "            \"cv_folds\": 3,\n",
        "            \"step\": \"baseline\",\n",
        "        }\n",
        "        _safe_add_model_params(model, params)\n",
        "\n",
        "        metrics = {\n",
        "            \"train_r2\": train_r2,\n",
        "            \"test_r2\": test_r2,\n",
        "            \"train_rmse\": train_rmse,\n",
        "            \"test_rmse\": test_rmse,\n",
        "            \"train_mae\": train_mae,\n",
        "            \"test_mae\": test_mae,\n",
        "            \"cv_r2_mean\": cv_mean,\n",
        "            \"cv_r2_std\": cv_std,\n",
        "            \"overfitting_score\": float(abs(train_r2 - test_r2)),\n",
        "            \"training_time_seconds\": training_time,\n",
        "            \"mape\": mape,\n",
        "        }\n",
        "\n",
        "        # log to MLflow (uses your enhanced logger defined in previous cell)\n",
        "        run_id = log_model_enhanced_professional(\n",
        "            model=model,\n",
        "            model_name=model_name,\n",
        "            params=params,\n",
        "            metrics=metrics,\n",
        "            X_train_sample=X_train_scaled,         # numpy OK; logger converts & uses feature_names_numeric\n",
        "            y_train_sample=y_train,\n",
        "            feature_names=feature_names_numeric,\n",
        "            experiment_id=experiment_id,\n",
        "            run_name_prefix=\"Baseline\",\n",
        "            log_datasets=True\n",
        "        )\n",
        "\n",
        "        baseline_results[model_name] = {\n",
        "            \"model\": model,\n",
        "            \"metrics\": metrics,\n",
        "            \"run_id\": run_id,\n",
        "            \"test_r2\": test_r2,\n",
        "            \"test_rmse\": test_rmse,\n",
        "        }\n",
        "\n",
        "        print(f\"  üìä Test R¬≤: {test_r2:.4f}\")\n",
        "        print(f\"  üìä Test RMSE: ${test_rmse:,.0f}\")\n",
        "        print(f\"  üìä CV R¬≤ (5-fold): {cv_mean:.4f} ¬± {cv_std:.4f}\")\n",
        "        print(f\"  ‚è±Ô∏è  Training time: {training_time:.2f} seconds\")\n",
        "\n",
        "    print(f\"\\n‚úÖ Step 1 Complete: {len(baseline_results)} baseline models trained\")\n",
        "    return baseline_results, X_train_scaled, X_test_scaled, y_train, y_test, scaler, feature_names_numeric\n",
        "\n",
        "# Run Step 1 (uses your DagsHub experiment id from the earlier setup)\n",
        "baseline_results, X_train_scaled, X_test_scaled, y_train, y_test, scaler, feature_names_numeric = step1_baseline_models(\n",
        "    X, y, feature_columns, experiments['baseline']\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Step 1 function ready!\")\n"
      ],
      "metadata": {
        "id": "46-Aqa_1YCYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step2"
      },
      "source": [
        "## 8. Step 2: Comprehensive Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```python\n",
        "# This is formatted as code\n",
        "def step2_hyperparameter_tuning(X_train_scaled, X_test_scaled, y_train, y_test,\n",
        "                               feature_names_numeric, experiment_id, baseline_results): # Changed feature_names to feature_names_numeric\n",
        "    \"\"\"Step 2: Comprehensive hyperparameter tuning with multiple methods\"\"\"\n",
        "    print(\"\\nüéØ STEP 2: COMPREHENSIVE HYPERPARAMETER TUNING\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Define comprehensive parameter grids\n",
        "    param_grids = {\n",
        "        'Ridge': {\n",
        "            'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],\n",
        "            'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag']\n",
        "        },\n",
        "        'Lasso': {\n",
        "            'alpha': [0.001, 0.01, 0.1, 1.0, 10.0],\n",
        "            'max_iter': [1000, 2000, 3000],\n",
        "            'tol': [1e-4, 1e-3, 1e-2]\n",
        "        },\n",
        "        'ElasticNet': {\n",
        "            'alpha': [0.001, 0.01, 0.1, 1.0, 10.0],\n",
        "            'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
        "            'max_iter': [1000, 2000, 3000]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Define tuning methods\n",
        "    tuning_methods = {\n",
        "        'GridSearchCV': GridSearchCV,\n",
        "        'RandomizedSearchCV': RandomizedSearchCV\n",
        "    }\n",
        "\n",
        "    tuning_results = {}\n",
        "\n",
        "    for model_name in ['Ridge', 'Lasso', 'ElasticNet']:\n",
        "        print(f\"\\nüîÑ Tuning {model_name} with multiple methods...\")\n",
        "\n",
        "        # Get base model\n",
        "        if model_name == 'Ridge':\n",
        "            base_model = Ridge(random_state=42)\n",
        "        elif model_name == 'Lasso':\n",
        "            base_model = Lasso(random_state=42)\n",
        "        elif model_name == 'ElasticNet':\n",
        "            base_model = ElasticNet(random_state=42)\n",
        "\n",
        "        best_method_result = None\n",
        "        best_score = -np.inf\n",
        "\n",
        "        # Try both tuning methods\n",
        "        for method_name, method_class in tuning_methods.items():\n",
        "            print(f\"  üìä Using {method_name}...\")\n",
        "\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Configure search method\n",
        "            if method_name == 'GridSearchCV':\n",
        "                search = method_class(\n",
        "                    base_model,\n",
        "                    param_grids[model_name],\n",
        "                    cv=5,\n",
        "                    scoring='r2',\n",
        "                    n_jobs=-1,\n",
        "                    verbose=0\n",
        "                )\n",
        "            else:  # RandomizedSearchCV\n",
        "                search = method_class(\n",
        "                    base_model,\n",
        "                    param_grids[model_name],\n",
        "                    n_iter=50,  # Limit iterations for efficiency\n",
        "                    cv=5,\n",
        "                    scoring='r2',\n",
        "                    n_jobs=-1,\n",
        "                    random_state=42,\n",
        "                    verbose=0\n",
        "                )\n",
        "\n",
        "            # Fit the search\n",
        "            search.fit(X_train_scaled, y_train)\n",
        "            tuning_time = time.time() - start_time\n",
        "\n",
        "            # Check if this is the best method so far\n",
        "            if search.best_score_ > best_score:\n",
        "                best_score = search.best_score_\n",
        "                best_method_result = {\n",
        "                    'method': method_name,\n",
        "                    'search': search,\n",
        "                    'tuning_time': tuning_time\n",
        "                }\n",
        "\n",
        "            print(f\"    ‚úì {method_name}: Best CV R¬≤ = {search.best_score_:.4f} (Time: {tuning_time:.1f}s)\")\n",
        "\n",
        "        # Use the best method's result\n",
        "        best_search = best_method_result['search']\n",
        "        best_model = best_search.best_estimator_\n",
        "        best_method = best_method_result['method']\n",
        "        total_tuning_time = best_method_result['tuning_time']\n",
        "\n",
        "        # Make predictions with best model\n",
        "        y_train_pred = best_model.predict(X_train_scaled)\n",
        "        y_test_pred = best_model.predict(X_test_scaled)\n",
        "\n",
        "        # Calculate metrics\n",
        "        train_r2 = r2_score(y_train, y_train_pred)\n",
        "        test_r2 = r2_score(y_test, y_test_pred)\n",
        "        train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "        test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "        train_mae = mean_absolute_error(y_train, y_train_pred)\n",
        "        test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "\n",
        "        # Calculate improvement vs baseline\n",
        "        baseline_r2 = baseline_results[model_name]['test_r2']\n",
        "        improvement_r2 = test_r2 - baseline_r2\n",
        "        improvement_pct = (improvement_r2 / baseline_r2) * 100\n",
        "\n",
        "        # Prepare parameters\n",
        "        params = {\n",
        "            'model_type': f'{model_name}_Tuned',\n",
        "            'tuning_method': best_method,\n",
        "            'cv_folds': 5,\n",
        "            'tuning_time_seconds': round(total_tuning_time, 2),\n",
        "            'baseline_r2': baseline_r2,\n",
        "            'improvement_r2': improvement_r2,\n",
        "            'improvement_percentage': improvement_pct,\n",
        "            'step': 'hyperparameter_tuning'\n",
        "        }\n",
        "\n",
        "        # Add best parameters\n",
        "        for param, value in best_search.best_params_.items():\n",
        "            params[f'best_{param}'] = value\n",
        "\n",
        "        # Prepare metrics\n",
        "        metrics = {\n",
        "            'train_r2': train_r2,\n",
        "            'test_r2': test_r2,\n",
        "            'train_rmse': train_rmse,\n",
        "            'test_rmse': test_rmse,\n",
        "            'train_mae': train_mae,\n",
        "            'test_mae': test_mae,\n",
        "            'cv_best_score': best_search.best_score_,\n",
        "            'overfitting_score': abs(train_r2 - test_r2),\n",
        "            'tuning_time': total_tuning_time,\n",
        "            'improvement_r2': improvement_r2,\n",
        "            'improvement_percentage': improvement_pct,\n",
        "            'mape': np.mean(np.abs((y_test - y_test_pred) / y_test)) * 100\n",
        "        }\n",
        "\n",
        "        # Log to MLflow\n",
        "        run_id = log_model_enhanced_professional(\n",
        "            model=best_model,\n",
        "            model_name=f'{model_name}_Tuned',\n",
        "            params=params,\n",
        "            metrics=metrics,\n",
        "            X_train_sample=X_train_scaled,\n",
        "            y_train_sample=y_train,\n",
        "            feature_names=feature_names_numeric, # Use numeric feature names for logging\n",
        "            experiment_id=experiment_id,\n",
        "            run_name_prefix=\"Tuned\"\n",
        "        )\n",
        "\n",
        "        tuning_results[model_name] = {\n",
        "            'model': best_model,\n",
        "            'best_params': best_search.best_params_,\n",
        "            'best_method': best_method,\n",
        "            'metrics': metrics,\n",
        "            'run_id': run_id,\n",
        "            'test_r2': test_r2,\n",
        "            'test_rmse': test_rmse,\n",
        "            'improvement_r2': improvement_r2,\n",
        "            'improvement_pct': improvement_pct\n",
        "        }\n",
        "\n",
        "        print(f\"  üèÜ Best {model_name}: R¬≤ = {test_r2:.4f} (‚Üë{improvement_r2:+.4f}, {improvement_pct:+.2f}%)\")\n",
        "        print(f\"    Method: {best_method} | Params: {best_search.best_params_}\")\n",
        "\n",
        "    print(f\"\\n‚úÖ Step 2 Complete: {len(tuning_results)} models optimized\")\n",
        "    return tuning_results\n",
        "\n",
        "# Note: Run this after Step 1\n",
        "tuning_results = step2_hyperparameter_tuning(\n",
        "     X_train_scaled, X_test_scaled, y_train, y_test, feature_names_numeric, # Added feature_names_numeric\n",
        "     experiments['tuning'], baseline_results\n",
        " )\n",
        "\n",
        "print(\"‚úÖ Step 2 function ready!\")\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "4Ey0NPruhHuK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "My record!\n",
        "\n",
        "---\n",
        "*   List item\n",
        "*   n_iter=50, cv=5,\n",
        "*   n_iter=30, cv=3,\n",
        "*   n_iter=25, cv=3\n",
        "\n"
      ],
      "metadata": {
        "id": "99ZlGjFAxXBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8) Step 2 ‚Äî Comprehensive Hyperparameter Tuning (RandomizedSearchCV + DagsHub-safe)\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import numpy as np, time\n",
        "\n",
        "def step2_randomized_tuning(\n",
        "    X_train_scaled, X_test_scaled, y_train, y_test,\n",
        "    feature_names_numeric, experiment_id, baseline_results,\n",
        "    n_iter=25, cv=3, random_state=42\n",
        "):\n",
        "    \"\"\"Step 2: Hyperparameter tuning using RandomizedSearchCV (logs to MLflow/DagsHub).\"\"\"\n",
        "    print(\"\\nüéØ STEP 2: RANDOMIZED HYPERPARAMETER TUNING\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Parameter distributions (kept compact for speed)\n",
        "    param_distributions = {\n",
        "        \"Ridge\": {\n",
        "            \"alpha\":   [0.001, 0.01, 0.1, 1.0, 10.0],\n",
        "            \"solver\":  [\"auto\", \"svd\", \"cholesky\", \"lsqr\", \"sag\", \"saga\"],\n",
        "            \"tol\":     [1e-4, 1e-3, 1e-2],\n",
        "            \"max_iter\":[None, 1000, 2000]\n",
        "        },\n",
        "        \"Lasso\": {\n",
        "            \"alpha\":   [0.001, 0.01, 0.1, 1.0, 10.0],\n",
        "            \"max_iter\":[1000, 2000, 3000],\n",
        "            \"tol\":     [1e-4, 1e-3],\n",
        "            \"selection\": [\"cyclic\", \"random\"]\n",
        "        },\n",
        "        \"ElasticNet\": {\n",
        "            \"alpha\":    [0.001, 0.01, 0.1, 1.0, 10.0],\n",
        "            \"l1_ratio\": [0.1, 0.3, 0.5, 0.7, 0.9],\n",
        "            \"max_iter\": [1000, 2000, 3000],\n",
        "            \"tol\":      [1e-4, 1e-3],\n",
        "            \"selection\":[\"cyclic\", \"random\"]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    tuning_results = {}\n",
        "\n",
        "    def safe_mape(y_true, y_pred):\n",
        "        denom = np.clip(np.asarray(y_true), 1e-8, None)\n",
        "        return float(np.mean(np.abs((y_true - y_pred) / denom)) * 100.0)\n",
        "\n",
        "    for model_name in [\"Ridge\", \"Lasso\", \"ElasticNet\"]:\n",
        "        print(f\"\\nüîÑ Tuning {model_name} with RandomizedSearchCV...\")\n",
        "\n",
        "        # Base models (use your linear family defaults)\n",
        "        if model_name == \"Ridge\":\n",
        "            base_model = Ridge(random_state=random_state)\n",
        "        elif model_name == \"Lasso\":\n",
        "            base_model = Lasso(random_state=random_state, max_iter=2000)\n",
        "        else:\n",
        "            base_model = ElasticNet(random_state=random_state, max_iter=2000)\n",
        "\n",
        "        search = RandomizedSearchCV(\n",
        "            estimator=base_model,\n",
        "            param_distributions=param_distributions[model_name],\n",
        "            n_iter=n_iter,\n",
        "            cv=cv,\n",
        "            scoring=\"r2\",\n",
        "            n_jobs=-1,\n",
        "            random_state=random_state,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        start_time = time.time()\n",
        "        search.fit(X_train_scaled, y_train)\n",
        "        tuning_time = float(time.time() - start_time)\n",
        "\n",
        "        best_model = search.best_estimator_\n",
        "        y_train_pred = best_model.predict(X_train_scaled)\n",
        "        y_test_pred  = best_model.predict(X_test_scaled)\n",
        "\n",
        "        # Metrics\n",
        "        train_r2   = float(r2_score(y_train, y_train_pred))\n",
        "        test_r2    = float(r2_score(y_test,  y_test_pred))\n",
        "        train_rmse = float(np.sqrt(mean_squared_error(y_train, y_train_pred)))\n",
        "        test_rmse  = float(np.sqrt(mean_squared_error(y_test,  y_test_pred)))\n",
        "        train_mae  = float(mean_absolute_error(y_train, y_train_pred))\n",
        "        test_mae   = float(mean_absolute_error(y_test,  y_test_pred))\n",
        "        mape       = safe_mape(y_test, y_test_pred)\n",
        "\n",
        "        # Improvement vs baseline\n",
        "        baseline_r2 = float(baseline_results[model_name][\"test_r2\"]) if model_name in baseline_results else 0.0\n",
        "        improvement_r2 = float(test_r2 - baseline_r2)\n",
        "        improvement_pct = float((improvement_r2 / baseline_r2) * 100.0) if abs(baseline_r2) > 1e-12 else float(\"nan\")\n",
        "\n",
        "        # Params & metrics for MLflow logging\n",
        "        params = {\n",
        "            \"model_type\": f\"{model_name}_Tuned\",\n",
        "            \"tuning_method\": \"RandomizedSearchCV\",\n",
        "            \"cv_folds\": int(cv),\n",
        "            \"tuning_time_seconds\": round(tuning_time, 2),\n",
        "            \"baseline_r2\": baseline_r2,\n",
        "            \"improvement_r2\": improvement_r2,\n",
        "            \"improvement_percentage\": improvement_pct,\n",
        "            \"step\": \"hyperparameter_tuning\"\n",
        "        }\n",
        "        for p, v in search.best_params_.items():\n",
        "            params[f\"best_{p}\"] = v\n",
        "\n",
        "        metrics = {\n",
        "            \"train_r2\": train_r2,\n",
        "            \"test_r2\":  test_r2,\n",
        "            \"train_rmse\": train_rmse,\n",
        "            \"test_rmse\":  test_rmse,\n",
        "            \"train_mae\":  train_mae,\n",
        "            \"test_mae\":   test_mae,\n",
        "            \"cv_best_score\": float(search.best_score_),\n",
        "            \"overfitting_score\": float(abs(train_r2 - test_r2)),\n",
        "            \"tuning_time\": tuning_time,\n",
        "            \"improvement_r2\": improvement_r2,\n",
        "            \"improvement_percentage\": improvement_pct,\n",
        "            \"mape\": mape\n",
        "        }\n",
        "\n",
        "        # Log tuned model to MLflow/DagsHub\n",
        "        run_id = log_model_enhanced_professional(\n",
        "            model=best_model,\n",
        "            model_name=f\"{model_name}_Tuned\",\n",
        "            params=params,\n",
        "            metrics=metrics,\n",
        "            X_train_sample=X_train_scaled,\n",
        "            y_train_sample=y_train,\n",
        "            feature_names=feature_names_numeric,\n",
        "            experiment_id=experiment_id,\n",
        "            run_name_prefix=\"Tuned\",\n",
        "            log_datasets=True\n",
        "        )\n",
        "\n",
        "        tuning_results[model_name] = {\n",
        "            \"model\": best_model,\n",
        "            \"best_params\": search.best_params_,\n",
        "            \"best_method\": \"RandomizedSearchCV\",\n",
        "            \"metrics\": metrics,\n",
        "            \"run_id\": run_id,\n",
        "            \"test_r2\": test_r2,\n",
        "            \"test_rmse\": test_rmse,\n",
        "            \"improvement_r2\": improvement_r2,\n",
        "            \"improvement_pct\": improvement_pct\n",
        "        }\n",
        "\n",
        "        print(f\"  üèÜ Best {model_name}: R¬≤ = {test_r2:.4f} (Œî {improvement_r2:+.4f}, {improvement_pct:+.2f}%)\")\n",
        "        print(f\"    Params: {search.best_params_} | Tuning time: {tuning_time:.1f}s\")\n",
        "\n",
        "    print(f\"\\n‚úÖ Step 2 Complete: {len(tuning_results)} models optimized with RandomizedSearchCV\")\n",
        "    return tuning_results\n",
        "\n",
        "# Run Step 2 (uses your Step 1 outputs and DagsHub experiment id)\n",
        "tuning_results = step2_randomized_tuning(\n",
        "    X_train_scaled, X_test_scaled, y_train, y_test,\n",
        "    feature_names_numeric, experiments[\"tuning\"], baseline_results,\n",
        "    n_iter=25, cv=3, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "UdK5hRSZaF3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step3"
      },
      "source": [
        "## 9. Step 3: Model Comparison & Selection"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9) Step 3 ‚Äî Model comparison & final selection (DagsHub/MLflow-safe)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import mlflow\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "\n",
        "def step3_model_comparison(baseline_results, tuning_results, X_train_scaled, X_test_scaled,\n",
        "                           y_train, y_test, feature_names_numeric, experiment_id):\n",
        "    \"\"\"Step 3: Comprehensive model comparison and final selection (logs summaries/artifacts to MLflow).\"\"\"\n",
        "    print(\"\\nüéØ STEP 3: MODEL COMPARISON & SELECTION\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # ---- collect results ----------------------------------------------------\n",
        "    all_results = {}\n",
        "\n",
        "    # Baselines\n",
        "    for name, result in baseline_results.items():\n",
        "        all_results[f\"{name}_Baseline\"] = {\n",
        "            \"model\": result[\"model\"],\n",
        "            \"test_r2\": float(result[\"test_r2\"]),\n",
        "            \"test_rmse\": float(result[\"test_rmse\"]),\n",
        "            \"type\": \"baseline\",\n",
        "            \"run_id\": result[\"run_id\"],\n",
        "            \"metrics\": result[\"metrics\"]\n",
        "        }\n",
        "\n",
        "    # Tuned\n",
        "    for name, result in tuning_results.items():\n",
        "        all_results[f\"{name}_Tuned\"] = {\n",
        "            \"model\": result[\"model\"],\n",
        "            \"test_r2\": float(result[\"test_r2\"]),\n",
        "            \"test_rmse\": float(result[\"test_rmse\"]),\n",
        "            \"type\": \"tuned\",\n",
        "            \"run_id\": result[\"run_id\"],\n",
        "            \"metrics\": result[\"metrics\"],\n",
        "            \"improvement_r2\": float(result.get(\"improvement_r2\", 0.0)),\n",
        "            \"improvement_pct\": float(result.get(\"improvement_pct\", 0.0)),\n",
        "            \"best_method\": result.get(\"best_method\", \"RandomizedSearchCV\")\n",
        "        }\n",
        "\n",
        "    # ---- pick best by test R2 ----------------------------------------------\n",
        "    best_model_name = max(all_results.keys(), key=lambda x: all_results[x][\"test_r2\"])\n",
        "    best_model_info = all_results[best_model_name]\n",
        "    best_model = best_model_info[\"model\"]\n",
        "\n",
        "    print(f\"üèÜ BEST OVERALL MODEL: {best_model_name}\")\n",
        "    print(f\"   üìà R¬≤ Score: {best_model_info['test_r2']:.4f}\")\n",
        "    print(f\"   üí∞ RMSE: ${best_model_info['test_rmse']:,.0f}\")\n",
        "    print(f\"   üîß Type: {best_model_info['type']}\")\n",
        "\n",
        "    # ---- comparison dataframe ----------------------------------------------\n",
        "    comparison_rows = []\n",
        "    for name, info in all_results.items():\n",
        "        row = {\n",
        "            \"Model\": name,\n",
        "            \"Type\": info[\"type\"],\n",
        "            \"R2_Score\": float(info[\"metrics\"].get(\"test_r2\", info[\"test_r2\"])),\n",
        "            \"RMSE\": float(info[\"metrics\"].get(\"test_rmse\", info[\"test_rmse\"])),\n",
        "            \"MAE\": float(info[\"metrics\"].get(\"test_mae\", 0.0)),\n",
        "            \"MAPE\": float(info[\"metrics\"].get(\"mape\", 0.0)),\n",
        "            \"CV_Score\": float(info[\"metrics\"].get(\"cv_r2_mean\", info[\"metrics\"].get(\"cv_best_score\", 0.0))),\n",
        "            \"Overfitting\": float(info[\"metrics\"].get(\"overfitting_score\", 0.0)),\n",
        "            \"Training_Time\": float(info[\"metrics\"].get(\"training_time_seconds\", info[\"metrics\"].get(\"tuning_time\", 0.0))),\n",
        "            \"R2_Improvement\": float(info.get(\"improvement_r2\", 0.0)),\n",
        "            \"Improvement_Pct\": float(info.get(\"improvement_pct\", 0.0)),\n",
        "            \"Method\": info.get(\"best_method\", \"Default\"),\n",
        "            \"Run_ID\": info[\"run_id\"]\n",
        "        }\n",
        "        comparison_rows.append(row)\n",
        "\n",
        "    comparison_df = pd.DataFrame(comparison_rows).sort_values(\"R2_Score\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "    # ---- final evaluation on best model -------------------------------------\n",
        "    def safe_mape(y_true, y_pred):\n",
        "        denom = np.clip(np.asarray(y_true), 1e-8, None)\n",
        "        return float(np.mean(np.abs((y_true - y_pred) / denom)) * 100.0)\n",
        "\n",
        "    y_train_pred = best_model.predict(X_train_scaled)\n",
        "    y_test_pred  = best_model.predict(X_test_scaled)\n",
        "\n",
        "    final_metrics = {\n",
        "        \"final_train_r2\": float(r2_score(y_train, y_train_pred)),\n",
        "        \"final_test_r2\":  float(r2_score(y_test,  y_test_pred)),\n",
        "        \"final_train_rmse\": float(np.sqrt(mean_squared_error(y_train, y_train_pred))),\n",
        "        \"final_test_rmse\":  float(np.sqrt(mean_squared_error(y_test,  y_test_pred))),\n",
        "        \"final_train_mae\": float(mean_absolute_error(y_train, y_train_pred)),\n",
        "        \"final_test_mae\":  float(mean_absolute_error(y_test,  y_test_pred)),\n",
        "        \"final_mape\": safe_mape(y_test, y_test_pred),\n",
        "        \"total_models_tested\": int(len(all_results)),\n",
        "        \"best_baseline_r2\": float(max([info[\"test_r2\"] for _, info in all_results.items() if info[\"type\"] == \"baseline\"])),\n",
        "        \"best_tuned_r2\": float(max([info[\"test_r2\"] for _, info in all_results.items() if info[\"type\"] == \"tuned\"]))\n",
        "                    if any(info[\"type\"] == \"tuned\" for info in all_results.values()) else float(\"nan\"),\n",
        "    }\n",
        "    final_metrics[\"overall_improvement\"] = float(\n",
        "        final_metrics[\"final_test_r2\"] -\n",
        "        max([info[\"test_r2\"] for _, info in all_results.items() if info[\"type\"] == \"baseline\"])\n",
        "    )\n",
        "\n",
        "    # ---- params for final model logging -------------------------------------\n",
        "    final_params = {\n",
        "        \"best_model_name\": best_model_name,\n",
        "        \"best_model_type\": best_model_info[\"type\"],\n",
        "        \"total_models_evaluated\": int(len(all_results)),\n",
        "        \"analysis_timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "        \"step\": \"final_comparison\"\n",
        "    }\n",
        "    if hasattr(best_model, \"get_params\"):\n",
        "        for p, v in best_model.get_params().items():\n",
        "            final_params[f\"best_model_{p}\"] = v\n",
        "\n",
        "    # ---- log best model again (for clear ‚ÄúFINAL‚Äù run) -----------------------\n",
        "    run_id = log_model_enhanced_professional(\n",
        "        model=best_model,\n",
        "        model_name=f\"FINAL_{best_model_name}\",\n",
        "        params=final_params,\n",
        "        metrics=final_metrics,\n",
        "        X_train_sample=X_train_scaled,\n",
        "        y_train_sample=y_train,\n",
        "        feature_names=feature_names_numeric,\n",
        "        experiment_id=experiment_id,\n",
        "        run_name_prefix=\"Final\",\n",
        "        log_datasets=True\n",
        "    )\n",
        "\n",
        "    # ---- comparison dashboard run ------------------------------------------\n",
        "    with mlflow.start_run(experiment_id=experiment_id, run_name=\"Model_Comparison_Dashboard\"):\n",
        "        # save comparison table\n",
        "        comp_csv = \"model_comparison_comprehensive.csv\"\n",
        "        comparison_df.to_csv(comp_csv, index=False)\n",
        "        mlflow.log_artifact(comp_csv)\n",
        "\n",
        "        # plots grid\n",
        "        fig = plt.figure(figsize=(20, 16))\n",
        "\n",
        "        # colors by type\n",
        "        bar_colors = [\"skyblue\" if (\"Baseline\" in m) else \"lightcoral\" for m in comparison_df[\"Model\"]]\n",
        "\n",
        "        # 1) R2\n",
        "        ax1 = plt.subplot(3, 3, 1)\n",
        "        b1 = ax1.bar(range(len(comparison_df)), comparison_df[\"R2_Score\"], color=bar_colors, alpha=0.85, edgecolor=\"black\")\n",
        "        ax1.set_title(\"Model R¬≤ Comparison\", fontsize=14, fontweight=\"bold\")\n",
        "        ax1.set_ylabel(\"R¬≤\")\n",
        "        ax1.set_xticks(range(len(comparison_df)))\n",
        "        ax1.set_xticklabels(comparison_df[\"Model\"], rotation=45, ha=\"right\")\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "        for bar in b1:\n",
        "            h = bar.get_height()\n",
        "            ax1.text(bar.get_x() + bar.get_width()/2., h + 0.001, f\"{h:.4f}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
        "\n",
        "        # 2) RMSE\n",
        "        ax2 = plt.subplot(3, 3, 2)\n",
        "        b2 = ax2.bar(range(len(comparison_df)), comparison_df[\"RMSE\"], color=bar_colors, alpha=0.85, edgecolor=\"black\")\n",
        "        ax2.set_title(\"Model RMSE Comparison\", fontsize=14, fontweight=\"bold\")\n",
        "        ax2.set_ylabel(\"RMSE ($)\")\n",
        "        ax2.set_xticks(range(len(comparison_df)))\n",
        "        ax2.set_xticklabels(comparison_df[\"Model\"], rotation=45, ha=\"right\")\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "\n",
        "        # 3) Improvement (tuned only)\n",
        "        ax3 = plt.subplot(3, 3, 3)\n",
        "        tuned_df = comparison_df[comparison_df[\"Type\"] == \"tuned\"]\n",
        "        if len(tuned_df) > 0:\n",
        "            b3 = ax3.bar(range(len(tuned_df)), tuned_df[\"Improvement_Pct\"], color=\"green\", alpha=0.85, edgecolor=\"black\")\n",
        "            ax3.set_title(\"Hyperparameter Tuning Improvement\", fontsize=14, fontweight=\"bold\")\n",
        "            ax3.set_ylabel(\"Improvement (%)\")\n",
        "            ax3.set_xticks(range(len(tuned_df)))\n",
        "            ax3.set_xticklabels([n.replace(\"_Tuned\", \"\") for n in tuned_df[\"Model\"]], rotation=45, ha=\"right\")\n",
        "            ax3.grid(True, alpha=0.3)\n",
        "\n",
        "        # 4) Time\n",
        "        ax4 = plt.subplot(3, 3, 4)\n",
        "        b4 = ax4.bar(range(len(comparison_df)), comparison_df[\"Training_Time\"], color=bar_colors, alpha=0.85, edgecolor=\"black\")\n",
        "        ax4.set_title(\"Training/Tuning Time Comparison\", fontsize=14, fontweight=\"bold\")\n",
        "        ax4.set_ylabel(\"Time (s)\")\n",
        "        ax4.set_xticks(range(len(comparison_df)))\n",
        "        ax4.set_xticklabels(comparison_df[\"Model\"], rotation=45, ha=\"right\")\n",
        "        ax4.grid(True, alpha=0.3)\n",
        "\n",
        "        # 5) MAPE\n",
        "        ax5 = plt.subplot(3, 3, 5)\n",
        "        b5 = ax5.bar(range(len(comparison_df)), comparison_df[\"MAPE\"], color=bar_colors, alpha=0.85, edgecolor=\"black\")\n",
        "        ax5.set_title(\"Mean Absolute Percentage Error\", fontsize=14, fontweight=\"bold\")\n",
        "        ax5.set_ylabel(\"MAPE (%)\")\n",
        "        ax5.set_xticks(range(len(comparison_df)))\n",
        "        ax5.set_xticklabels(comparison_df[\"Model\"], rotation=45, ha=\"right\")\n",
        "        ax5.grid(True, alpha=0.3)\n",
        "\n",
        "        # 6) Overfitting\n",
        "        ax6 = plt.subplot(3, 3, 6)\n",
        "        b6 = ax6.bar(range(len(comparison_df)), comparison_df[\"Overfitting\"], color=bar_colors, alpha=0.85, edgecolor=\"black\")\n",
        "        ax6.set_title(\"Overfitting Score\", fontsize=14, fontweight=\"bold\")\n",
        "        ax6.set_ylabel(\"|Train R¬≤ - Test R¬≤|\")\n",
        "        ax6.set_xticks(range(len(comparison_df)))\n",
        "        ax6.set_xticklabels(comparison_df[\"Model\"], rotation=45, ha=\"right\")\n",
        "        ax6.grid(True, alpha=0.3)\n",
        "\n",
        "        # 7) Radar\n",
        "        ax7 = plt.subplot(3, 3, 7, projection=\"polar\")\n",
        "        def safe_norm_inv(x):\n",
        "            xmax = np.nanmax(x) if len(x) else 0.0\n",
        "            return (1 - (x / xmax)) if xmax > 1e-12 else np.ones_like(x)\n",
        "        metrics_for_radar = {\n",
        "            \"R¬≤\": comparison_df[\"R2_Score\"].values,\n",
        "            \"Low RMSE\": safe_norm_inv(comparison_df[\"RMSE\"].values),\n",
        "            \"Low MAE\":  safe_norm_inv(comparison_df[\"MAE\"].values),\n",
        "            \"Low MAPE\": safe_norm_inv(comparison_df[\"MAPE\"].values),\n",
        "            \"CV Score\": comparison_df[\"CV_Score\"].values,\n",
        "            \"Stability\": safe_norm_inv(comparison_df[\"Overfitting\"].values)\n",
        "        }\n",
        "        angles = np.linspace(0, 2*np.pi, len(metrics_for_radar), endpoint=False)\n",
        "        angles = np.concatenate((angles, [angles[0]]))\n",
        "        colors_radar = [\"blue\", \"red\", \"green\", \"orange\"]\n",
        "        for i in range(min(4, len(comparison_df))):\n",
        "            vals = [metrics_for_radar[k][i] for k in metrics_for_radar.keys()]\n",
        "            vals += [vals[0]]\n",
        "            ax7.plot(angles, vals, \"o-\", linewidth=2, label=comparison_df.iloc[i][\"Model\"],\n",
        "                     color=colors_radar[i], alpha=0.9)\n",
        "            ax7.fill(angles, vals, alpha=0.1, color=colors_radar[i])\n",
        "        ax7.set_xticks(angles[:-1])\n",
        "        ax7.set_xticklabels(list(metrics_for_radar.keys()))\n",
        "        ax7.set_ylim(0, 1)\n",
        "        ax7.set_title(\"Top 4 Models Performance Radar\", fontsize=14, fontweight=\"bold\", pad=20)\n",
        "        ax7.legend(loc=\"upper right\", bbox_to_anchor=(1.3, 1.0))\n",
        "\n",
        "        # 8) Summary box\n",
        "        ax8 = plt.subplot(3, 3, 8); ax8.axis(\"off\")\n",
        "        summary_text = f\"\"\"\n",
        "üèÜ BEST MODEL: {best_model_name}\n",
        "üìà R¬≤ Score: {final_metrics['final_test_r2']:.4f}\n",
        "üí∞ RMSE: ${final_metrics['final_test_rmse']:,.0f}\n",
        "üìä MAE: ${final_metrics['final_test_mae']:,.0f}\n",
        "üìâ MAPE: {final_metrics['final_mape']:.2f}%\n",
        "\n",
        "üìä SUMMARY:\n",
        "‚Ä¢ Total Models: {final_metrics['total_models_tested']}\n",
        "‚Ä¢ Best Baseline R¬≤: {final_metrics['best_baseline_r2']:.4f}\n",
        "‚Ä¢ Best Tuned R¬≤: {final_metrics['best_tuned_r2'] if not np.isnan(final_metrics['best_tuned_r2']) else 'N/A'}\n",
        "‚Ä¢ Overall Improvement: {final_metrics['overall_improvement']:.4f}\n",
        "\n",
        "üéØ STUDENT: Aung Hlaing Tun (6319250G)\n",
        "üìÖ DATE: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\"\"\"\n",
        "        ax8.text(0.02, 0.98, summary_text, transform=ax8.transAxes, fontsize=11,\n",
        "                 va=\"top\", bbox=dict(boxstyle=\"round\", facecolor=\"lightblue\", alpha=0.8))\n",
        "\n",
        "        # 9) Ranking table (Top 5)\n",
        "        ax9 = plt.subplot(3, 3, 9); ax9.axis(\"tight\"); ax9.axis(\"off\")\n",
        "        topN = min(5, len(comparison_df))\n",
        "        ranking_data = []\n",
        "        for i in range(topN):\n",
        "            row = comparison_df.iloc[i]\n",
        "            ranking_data.append([f\"#{i+1}\", row[\"Model\"].replace(\"_\", \" \"),\n",
        "                                 f\"{row['R2_Score']:.4f}\", f\"${row['RMSE']:,.0f}\", f\"{row['MAPE']:.2f}%\"])\n",
        "        table = ax9.table(cellText=ranking_data,\n",
        "                          colLabels=[\"Rank\", \"Model\", \"R¬≤\", \"RMSE\", \"MAPE\"],\n",
        "                          cellLoc=\"center\", loc=\"center\")\n",
        "        table.auto_set_font_size(False); table.set_fontsize(9); table.scale(1.2, 1.5)\n",
        "        for i in range(topN + 1):\n",
        "            for j in range(5):\n",
        "                if i == 0:\n",
        "                    table[(i, j)].set_facecolor(\"#4CAF50\"); table[(i, j)].set_text_props(weight=\"bold\", color=\"white\")\n",
        "                elif i == 1:\n",
        "                    table[(i, j)].set_facecolor(\"#FFD700\")\n",
        "                else:\n",
        "                    table[(i, j)].set_facecolor(\"#F5F5F5\")\n",
        "        ax9.set_title(\"Model Ranking (Top 5)\", fontsize=14, fontweight=\"bold\")\n",
        "\n",
        "        plt.suptitle(\"HDB Resale Price Prediction - Comprehensive Model Analysis\\nEnhanced 3-Step Workflow\",\n",
        "                     fontsize=16, fontweight=\"bold\", y=0.98)\n",
        "        plt.tight_layout(); plt.subplots_adjust(top=0.93)\n",
        "        dash_png = \"comprehensive_model_dashboard.png\"\n",
        "        plt.savefig(dash_png, dpi=300, bbox_inches=\"tight\")\n",
        "        mlflow.log_artifact(dash_png)\n",
        "        plt.show(); plt.close()\n",
        "\n",
        "        # cleanup\n",
        "        for f in [comp_csv, dash_png]:\n",
        "            try: os.remove(f)\n",
        "            except Exception: pass\n",
        "\n",
        "    final_results = {\n",
        "        \"best_model\": best_model,\n",
        "        \"best_model_name\": best_model_name,\n",
        "        \"comparison_df\": comparison_df,\n",
        "        \"final_metrics\": final_metrics,\n",
        "        \"run_id\": run_id\n",
        "    }\n",
        "\n",
        "    print(\"\\nüìä FINAL RESULTS:\")\n",
        "    print(f\"   üèÜ Best Model: {best_model_name}\")\n",
        "    print(f\"   üìà Final R¬≤: {final_metrics['final_test_r2']:.4f}\")\n",
        "    print(f\"   üí∞ Final RMSE: ${final_metrics['final_test_rmse']:,.0f}\")\n",
        "    print(f\"   üìä Final MAPE: {final_metrics['final_mape']:.2f}%\")\n",
        "    print(f\"   üî¢ Total Models Tested: {final_metrics['total_models_tested']}\")\n",
        "    print(f\"   üìà Overall Improvement vs Best Baseline: {final_metrics['overall_improvement']:.4f}\")\n",
        "\n",
        "    print(\"\\n‚úÖ Step 3 Complete: Comprehensive analysis finished!\")\n",
        "    return final_results\n",
        "\n",
        "# Run Step 3\n",
        "final_results = step3_model_comparison(\n",
        "    baseline_results, tuning_results, X_train_scaled, X_test_scaled,\n",
        "    y_train, y_test, feature_names_numeric, experiments[\"comparison\"]\n",
        ")\n",
        "print(\"‚úÖ Step 3 function ready!\")\n"
      ],
      "metadata": {
        "id": "1Oiz0Z9SfxKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analysis for model overfitting**\n",
        "\n",
        "The code the overfitting score is defined as the absolute difference between the training and test R¬≤ values:\n",
        "\n",
        "\n",
        "```python\n",
        "\n",
        "'overfitting_score': abs(train_r2 - test_r2)\n",
        "\n",
        "```\n",
        "\n",
        "understanding for overfitting check: A model is considered to be overfitting when it performs much better on the training data than on unseen test data. In other words, if train_r2 is high and test_r2 is significantly lower, the abs(train_r2 - test_r2) term becomes large. That would indicate the model is memorising the training set and failing to generalise.\n",
        "\n",
        "In my results the ‚Äúoverfitting score‚Äù bars are extremely small (around 0.00008 in the chart). This means the difference between training and test R¬≤ is nearly zero, so the models‚Äô performance on training and test data is almost identical. Rather than signalling overfitting, the small values suggest that none of the six models are overfitting ‚Äì they are all generalising well to unseen data.\n",
        "\n"
      ],
      "metadata": {
        "id": "NgRUKDd66MzA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "complete_workflow"
      },
      "source": [
        "## 10. Complete 3-Step Workflow Execution"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10) Execute Complete 3-Step Workflow (wired for DagsHub + our Step2 name)\n",
        "from datetime import datetime\n",
        "import mlflow\n",
        "\n",
        "def execute_complete_workflow(X, y, feature_names):\n",
        "    \"\"\"Execute the complete 3-step enhanced workflow (Baseline ‚Üí Tuning ‚Üí Comparison).\"\"\"\n",
        "    print(\"üöÄ EXECUTING COMPLETE 3-STEP ENHANCED WORKFLOW\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"üìä HDB Resale Price Prediction - Enhanced Model Training\")\n",
        "    print(\"üë®‚Äçüéì Student: Aung Hlaing Tun (6319250G)\")\n",
        "    print(\"üìÖ Date:\", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Ensure experiment IDs exist (use global `experiments` if available, else look up)\n",
        "    def _exp_id(name, key=None):\n",
        "        if 'experiments' in globals() and key in experiments:\n",
        "            return experiments[key]\n",
        "        exp = mlflow.get_experiment_by_name(name)\n",
        "        return exp.experiment_id if exp else mlflow.create_experiment(name)\n",
        "\n",
        "    exp_baseline   = _exp_id(\"HDB_Step1_Baseline_Models\",     \"baseline\")\n",
        "    exp_tuning     = _exp_id(\"HDB_Step2_Hyperparameter_Tuning\",\"tuning\")\n",
        "    exp_compare    = _exp_id(\"HDB_Step3_Model_Comparison\",     \"comparison\")\n",
        "\n",
        "    # Step 1: Baseline Models\n",
        "    print(\"\\nüéØ EXECUTING STEP 1: BASELINE MODELS\")\n",
        "    baseline_results, X_train_scaled, X_test_scaled, y_train, y_test, scaler, feature_names_numeric = step1_baseline_models(\n",
        "        X, y, feature_names, exp_baseline\n",
        "    )\n",
        "\n",
        "    # Step 2: Hyperparameter Tuning (use our RandomizedSearch version)\n",
        "    print(\"\\nüéØ EXECUTING STEP 2: HYPERPARAMETER TUNING\")\n",
        "    tuning_results = step2_randomized_tuning(\n",
        "        X_train_scaled, X_test_scaled, y_train, y_test,\n",
        "        feature_names_numeric, exp_tuning, baseline_results,\n",
        "        n_iter=25, cv=3, random_state=42\n",
        "    )\n",
        "\n",
        "    # Step 3: Model Comparison\n",
        "    print(\"\\nüéØ EXECUTING STEP 3: MODEL COMPARISON\")\n",
        "    final_results = step3_model_comparison(\n",
        "        baseline_results, tuning_results,\n",
        "        X_train_scaled, X_test_scaled, y_train, y_test,\n",
        "        feature_names_numeric, exp_compare\n",
        "    )\n",
        "\n",
        "    # Final Summary\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"üéâ COMPLETE 3-STEP WORKFLOW FINISHED!\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    print(f\"\\nüìä WORKFLOW SUMMARY:\")\n",
        "    print(f\"   ‚úÖ Step 1: {len(baseline_results)} baseline models trained\")\n",
        "    print(f\"   ‚úÖ Step 2: {len(tuning_results)} models optimized with hyperparameter tuning\")\n",
        "    print(f\"   ‚úÖ Step 3: Comprehensive comparison and selection completed\")\n",
        "\n",
        "    # DagsHub MLflow UI link\n",
        "    uri = mlflow.get_tracking_uri()\n",
        "    ui_link = f\"{uri}/#/experiments\" if uri.startswith(\"https://\") else \"Open your MLflow UI\"\n",
        "\n",
        "    print(f\"\\nüèÜ FINAL BEST MODEL:\")\n",
        "    print(f\"   Model: {final_results['best_model_name']}\")\n",
        "    print(f\"   R¬≤ Score: {final_results['final_metrics']['final_test_r2']:.4f}\")\n",
        "    print(f\"   RMSE: ${final_results['final_metrics']['final_test_rmse']:,.0f}\")\n",
        "    print(f\"   MAPE: {final_results['final_metrics']['final_mape']:.2f}%\")\n",
        "\n",
        "    print(f\"\\nüåê MLflow Dashboard: {ui_link}\")\n",
        "    print(f\"üìä Total Experiments: 3 (Baseline, Tuning, Comparison)\")\n",
        "    print(f\"üìà Total Models Logged: {final_results['final_metrics']['total_models_tested']}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"‚úÖ READY FOR TEAM COLLABORATION!\")\n",
        "    print(\"üìã Next Steps: Tree Models ‚Üí Boosting Models ‚Üí Ensemble\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    return {\n",
        "        \"baseline_results\": baseline_results,\n",
        "        \"tuning_results\": tuning_results,\n",
        "        \"final_results\": final_results,\n",
        "        \"data_splits\": (X_train_scaled, X_test_scaled, y_train, y_test),\n",
        "        \"scaler\": scaler,\n",
        "        \"feature_names_numeric\": feature_names_numeric\n",
        "    }\n",
        "\n",
        "# Execute the complete workflow\n",
        "workflow_results = execute_complete_workflow(X, y, feature_columns)\n",
        "\n",
        "print(\"‚úÖ Complete workflow function ready!\")\n",
        "print(\"\\nüìù TO RUN THE COMPLETE WORKFLOW:\")\n",
        "print(\"   1) Load your preprocessed data (X, y, feature_columns)\")\n",
        "print(\"   2) Run: workflow_results = execute_complete_workflow(X, y, feature_columns)\")\n",
        "print(\"   3) Check the DagsHub MLflow UI for results\")\n"
      ],
      "metadata": {
        "id": "9UnDS1d5gqP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# This is formatted as code for log file store\n",
        "```python\n",
        "\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# path to the directory you want to download\n",
        "source_dir = '/content/mlflow_runs'  # adjust if your logs are in a different folder\n",
        "zip_path = '/content/mlflow_runs.zip'  # where to write the archive\n",
        "\n",
        "# make a ZIP archive of the directory\n",
        "shutil.make_archive(base_name=zip_path.replace('.zip', ''), format='zip', root_dir=source_dir)\n",
        "\n",
        "# initiate a download to your local machine\n",
        "files.download(zip_path)\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "vSRipY0ngK_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This is formatted as code that for best model save for deployment\n",
        "\n",
        "```python\n",
        "\n",
        "import joblib\n",
        "\n",
        "# After running the workflow:\n",
        "workflow_results = execute_complete_workflow(X, y, feature_columns)\n",
        "\n",
        "# Grab the trained model object\n",
        "best_model = workflow_results['final_results']['best_model']\n",
        "\n",
        "# Define where to save it (e.g. to your Google Drive)\n",
        "save_path = '/content/drive/MyDrive/hdb_best_model.pkl'\n",
        "\n",
        "# Serialize and save the model\n",
        "joblib.dump(best_model, save_path)\n",
        "print(f\"‚úÖ Best model saved to {save_path}\")\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "qUttU3YRf6Ul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìã Enhanced Instructions for Use\n",
        "\n",
        "### **How to Run This Enhanced Notebook:**\n",
        "\n",
        "1. **Setup Environment:**\n",
        "   - Keep **MLflow at 2.x** (DagsHub requires MLflow 2.x)\n",
        "   - Add your **DagsHub token** in Colab Secrets (key: `dagshub`)\n",
        "   - Point MLflow tracking to your **DagsHub repo** (no ngrok / Drive needed)\n",
        "\n",
        "2. **Load Data:**\n",
        "   - Use the processed HDB dataset from your previous notebook\n",
        "   - Ensure you have: **X** (features), **y** (target), **feature_columns** (names)\n",
        "\n",
        "3. **Execute 3-Step Workflow:**\n",
        "   ```python\n",
        "   # Run the complete workflow\n",
        "   workflow_results = execute_complete_workflow(X, y, feature_columns)\n",
        "\n",
        "### üéØ **Enhanced Features:**\n",
        "\n",
        "- ‚úÖ **3-Step Workflow:** Baseline ‚Üí Tuning ‚Üí Comparison  \n",
        "- ‚úÖ **Hyperparameter Tuning:** RandomizedSearchCV for Ridge, Lasso, ElasticNet  \n",
        "- ‚úÖ **Professional MLflow Integration (DagsHub):** Clean params/metrics, artifacts, model registry  \n",
        "- ‚úÖ **Dataset Logging:** Sampled inputs/targets saved as artifacts for lineage  \n",
        "- ‚úÖ **Comprehensive Visualizations:** 9-panel dashboard with radar charts  \n",
        "- ‚úÖ **Overfitting Check:** |Train R¬≤ ‚àí Test R¬≤| logged per model  \n",
        "- ‚úÖ **Team Collaboration Ready:** Experiments structured as Step 1 / Step 2 / Step 3\n",
        "\n",
        "---\n",
        "\n",
        "### üìä **Expected Enhanced Results (from this run):**\n",
        "\n",
        "- **Baseline Models:** ~**0.969** R¬≤ (Linear, Ridge, Lasso) ‚Äî ElasticNet baseline ‚âà **0.849**  \n",
        "- **Tuned Models:** ~**0.969** R¬≤ (ElasticNet improves to match; Ridge/Lasso near-optimal)  \n",
        "- **Best Method Selection:** **Ridge (Tuned)** via RandomizedSearchCV  \n",
        "- **Comprehensive Comparison:** **7 models** evaluated (4 baselines + 3 tuned)  \n",
        "- **Performance Improvement:** Minimal for Ridge/Lasso; **significant for ElasticNet** (~+14% relative vs its baseline)\n",
        "\n",
        "---\n",
        "\n",
        "### üîÑ **Next Steps for Team:**\n",
        "\n",
        "1. **Tree Models:** Decision Tree, Random Forest with hyperparameter tuning  \n",
        "2. **Boosting Models:** XGBoost, LightGBM, AdaBoost with optimization  \n",
        "3. **Ensemble Methods:** Stacking, blending, voting regressors  \n",
        "4. **Final Model Selection:** Compare all approaches and select best single or ensemble model  \n",
        "5. **Model Deployment:** Use MLflow **Model Registry** + simple Streamlit UI for serving\n",
        "\n",
        "- **Student:** Aung Hlaing Tun (6319250G)  \n",
        "- **Course:** NYP ITI-105 Machine Learning Project\n",
        "- **Project:** HDB Resale Price Prediction\n",
        "- **Version:** Enhanced Pro v3.0 with Comprehensive Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "yojL61CZnsIg"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}